{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# RAG Strategy\n",
        "\n",
        "## Strategy of the RAG System\n",
        "\n",
        "## Data Ingestion\n",
        "\n",
        "**Objective**: Efficiently extract text and detailed descriptions from documents filled with charts, diagrams, and images using Vision LLM, optimizing for time, cost, and accuracy.\n",
        "\n",
        "As the input document has a complex, Power-Point like a layout. So i decided to use the Vision LLMs to extract data instead of OCR, Currently OCR's are unable to fetch data in correct layout, they fetch text line by line, which missed the exact layout or sequence of text.\n",
        "\n",
        "1. **Conversion to Images**:\n",
        "   - Convert each page of the document into an image format. This is essential for processing visual elements such as charts and diagrams effectively.\n",
        "   \n",
        "2. **Batch Processing**:\n",
        "  - Process these images in batches to reduce time and cost. By batching the images, the system can handle multiple pages simultaneously, reducing the overall processing time.\n",
        "  - Utilize asynchronous function calls to run the LLM concurrently, extracting data from each batch simultaneously. This further reduces latency. Asynchronous processing allows multiple requests to be handled at the same time, speeding up the extraction process significantly.\n",
        "\n",
        "3. **Detailed Descriptions**:\n",
        "  - Request the LLM to generate detailed descriptions for each image. These descriptions provide additional context and details about the visual content.\n",
        "  - These descriptions help refine extracted data, especially for complex elements like tables which might not be correctly formatted initially. If the initial extraction misses certain details or formats, the detailed descriptions can provide the necessary context to correct and enhance the extracted data.\n",
        "\n",
        "\n",
        "#### Splitting Document into Smaller Chunks\n",
        "\n",
        "**Objective**: Maintain context within chunks while ensuring they are of a manageable size for embedding generation and further processing.\n",
        "\n",
        "1. **Recursive Character Text Splitter**:\n",
        "   - Utilize Langchain’s RecursiveCharacterTextSplitter with a token limit and priority separators [\"\\n\\n\", \".\", \"\\n\"].\n",
        "   - This ensures chunks are split at logical points, maintaining context and coherence within each chunk.\n",
        "   - Through experimentation, a chunk size of 100 tokens was selected. This size was found to balance the need for context with the need to manage chunk size for embedding purposes. It helps in maintaining some context in each chunk, making it easier for the model to understand and process the information.\n",
        "\n",
        "#### Embedding Generation and Upserting Vectors into Vector DB\n",
        "\n",
        "**Objective**: Generate and store contextual embeddings for each chunk, ensuring efficient and accurate retrieval during inference.\n",
        "\n",
        "1. **Embedding Generation**:\n",
        "   - Generate embeddings for each chunk using the \"voyage-large-2-instruct\" model from voyageai, known for its performance in general-purpose tasks. This model provides high-quality embeddings that capture the context and meaning of the text.\n",
        "   \n",
        "2. **Metadata Storage**:\n",
        "   - Store chunk_id, page_no, chunk_text, and page_description as metadata for each chunk. This metadata is essential for organizing and retrieving the chunks efficiently during the inference stage.\n",
        "   \n",
        "3. **Vector Database**:\n",
        "   - Used Pinecone for vector storage due to its low latency and hybrid search functionality. Pinecone’s capabilities ensure that the embeddings can be retrieved quickly and accurately, supporting real-time query processing.\n",
        "   \n",
        "4. **Future Improvements**:\n",
        "   - Generate sparse embeddings of the chunks and contextual embeddings of image descriptions. This dual embedding approach can enhance the retrieval accuracy by providing multiple perspectives on the data.\n",
        "  - Experiment with different combinations to determine the best retrieval accuracy. This iterative process will help in finding the optimal embedding strategy for various types of data.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Inference\n",
        "\n",
        "**Objective**: Efficiently retrieve relevant information in response to a query and generate accurate answers using the contextual data.\n",
        "\n",
        "1. **Query Embedding**:\n",
        "   - Generate embeddings for the input query using same embedding mode. This step ensures that the query is represented in the same vector space as the chunks, facilitating accurate similarity calculations.\n",
        "   \n",
        "2. **Chunk Retrieval**:\n",
        "   - Retrieve the top 5 relevant chunks using cosine similarity for similarity calculations, optimized for voyageai models.\n",
        "  - Filter out relevant chunks by comparing the similarity score with a pre-selected threshold (68% similarity). This threshold, determined through experimentation, ensures that only highly relevant chunks are considered for the final answer.\n",
        "   \n",
        "3. **Metadata Extraction**:\n",
        "   - For the top 3 vectors that pass the similarity score filter, extract chunk text, page number, page description, and the image of the most relevant chunks. This comprehensive extraction ensures that all necessary context is available for answer generation.\n",
        "   \n",
        "4. **Structured Input for Model**:\n",
        "   - Pass all this information in a well-structured format to the prompt.\n",
        "   - Ensure the model receives comprehensive details related to the query, allowing it to derive context from both the chunk and the associated image and description.\n",
        "   \n",
        "5. **Answer Generation**:\n",
        "   - Use GPT-4O for generating answers from the relevant context. This model has been selected for its superior performance in generating detailed and accurate responses based on the provided context.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation Strategy\n",
        "\n",
        "#### Objective\n",
        "To ensure the generated answers are accurate, relevant, and free from hallucinations, a two-level evaluation strategy is employed. This strategy utilizes both automated similarity checks and manual review through a judge LLM.\n",
        "\n",
        "#### Two-Level Evaluation\n",
        "\n",
        "**1. Cosine Similarity Check**\n",
        "- **Embedding Generation**:\n",
        "  - Generate embeddings for both the generated answer and the retrieved relevant context. These embeddings capture the semantic meaning of the text, allowing for an accurate comparison.\n",
        "  \n",
        "- **Similarity Measurement**:\n",
        "  - Measure the cosine similarity between the embeddings of the generated answer and the relevant context. Cosine similarity provides a metric for how closely the two vectors align, indicating the relevance of the generated answer to the context.\n",
        "  \n",
        "- **Threshold Check**:\n",
        "  - Compare the similarity score against a pre-defined threshold. This threshold is determined through experimentation to balance precision and recall. If the similarity score exceeds the threshold, the answer is considered relevant and is passed to the end user.\n",
        "  \n",
        "- **Answer Re-generation**:\n",
        "  - If the similarity score does not meet the threshold, tweak the inference prompt slightly and add more context. This iterative approach helps in refining the answer by providing additional information to the model.\n",
        "  \n",
        "**2. Judge LLM Verification**\n",
        "- **Manual Review by Judge LLM**:\n",
        "  - If the first evaluation check fails, pass the generated answer to a judge LLM. This LLM is a smaller, efficient model specifically designed for evaluating the relevance and accuracy of the generated answers.\n",
        "  \n",
        "- **Context and Image Verification**:\n",
        "  - Feed the judge LLM with the generated answer and the relevant context, potentially including images. This comprehensive input ensures the judge LLM has all necessary information to evaluate the answer accurately.\n",
        "  \n",
        "- **Hallucination and Relevance Check**:\n",
        "  - The judge LLM checks the answer for hallucinations and verifies its relevance to the provided context. It ensures that the answer is not only accurate but also directly related to the user's query and the given context.\n",
        "  \n",
        "- **Final Decision**:\n",
        "  - Based on the judge LLM’s evaluation, decide whether to pass the answer to the end user or to re-generate the answer with additional context. This step provides an additional layer of assurance, ensuring high-quality responses.\n"
      ],
      "metadata": {
        "id": "9mvo8IMYlhTJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Install dependencies"
      ],
      "metadata": {
        "id": "e2ecT4hzoLZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install all dependencies\n",
        "!pip install pdfplumber langchain pinecone tiktoken voyageai"
      ],
      "metadata": {
        "id": "E1-4cCc75kOT"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ddMIE1wo5RVa"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import requests\n",
        "import pdfplumber\n",
        "import json\n",
        "import base64\n",
        "import logging\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import tiktoken\n",
        "import uuid\n",
        "from pinecone import Pinecone\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import aiohttp\n",
        "import voyageai\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###To run this notebook, you need to set the following API keys in Google Colab Secrets:\n",
        "\n",
        "- OpenAI API Key\n",
        "- VoyageAI API Key\n",
        "- Pinecone API Key"
      ],
      "metadata": {
        "id": "kEJc5H85a876"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')\n",
        "VOYAGEAI_API_KEY = userdata.get('VOYAGEAI_API_KEY')\n",
        "\n",
        "#OR\n",
        "# OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')\n",
        "# PINECONE_API_KEY = os.environ.get('PINECONE_API_KEY')\n",
        "# VOYAGEAI_API_KEY = os.environ.get('VOYAGEAI_API_KEY')\n"
      ],
      "metadata": {
        "id": "ti45sRtybx4F"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "ZDR1VYi95RVb"
      },
      "outputs": [],
      "source": [
        "#To access the asyncio functionality in notebook\n",
        "nest_asyncio.apply()\n",
        "\n",
        "#Initializing the logger\n",
        "logger = logging.getLogger(__name__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Jnj6klP55RVb"
      },
      "outputs": [],
      "source": [
        "#Initializing the pinecone\n",
        "PINECONE_API_KEY = PINECONE_API_KEY\n",
        "PINECONE_ENV = \"us-east-1\"\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ws-VSG_5RVb",
        "outputId": "70807415-5d2d-443c-8381-8a8b11f4c8f5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1024,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'Investment_Case_For_Disruptive_Innovation': {'vector_count': 91},\n",
              "                'investment_case_for_disruptive_innovation': {'vector_count': 100},\n",
              "                'tifin-voyageai': {'vector_count': 100}},\n",
              " 'total_vector_count': 291}"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "#selecting the index\n",
        "index_name = \"tifin-voyage\"\n",
        "# connect to index\n",
        "index = pc.Index(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Ingestion"
      ],
      "metadata": {
        "id": "vkWHLGqpCVeH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "C75E59KI5RVb"
      },
      "outputs": [],
      "source": [
        "def remove_duplicates_preserve_order(input_list):\n",
        "    seen = set()\n",
        "    unique_list = [x for x in input_list if not (x in seen or seen.add(x))]\n",
        "    return unique_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Ingestion Utils"
      ],
      "metadata": {
        "id": "bnCMSZWDCdbi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "g2wZdiC75RVb"
      },
      "outputs": [],
      "source": [
        "def read_text_file(file_path):\n",
        "    \"\"\"\n",
        "    Read a text file\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(file_path, \"r\") as file:\n",
        "            content = file.read()\n",
        "        return content\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"Error: File '{file_path}' not found.\")\n",
        "    except Exception:\n",
        "        logger.error(\"An error occurred while reading the file.\")\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "    try:\n",
        "        with open(image_path, \"rb\") as image_file:\n",
        "            return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "    except (FileNotFoundError, PermissionError, IOError) as e:\n",
        "        # Handle the error or log it\n",
        "        logger.error(f\"Error reading file {image_path}: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VYY-HBHx5RVb"
      },
      "outputs": [],
      "source": [
        "async def LLM_call(images_payload):\n",
        "    \"\"\"\n",
        "    Makes an asynchronous call to the OpenAI API with the given payload and processes the response.\n",
        "\n",
        "    Args:\n",
        "        images_payload (str): The payload containing the input images to be sent to the OpenAI API.\n",
        "\n",
        "    Returns:\n",
        "        str: The processed response from the OpenAI API. If an error occurs, returns \"Failed to Transcribe\".\n",
        "    \"\"\"\n",
        "\n",
        "    url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    # Model to be used for the API call\n",
        "    model = \"gpt-4o-mini\"\n",
        "    # Fetch the OpenAI API key from user data\n",
        "    api_key = OPENAI_API_KEY\n",
        "\n",
        "    # Prepare the payload for the API call\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"response_format\": {\"type\": \"json_object\"},\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": images_payload}],\n",
        "        \"max_tokens\": 4000\n",
        "    }\n",
        "\n",
        "    # Set the headers for the API request\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    # Make an asynchronous POST request to the OpenAI API\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        async with session.post(url, headers=headers, json=payload) as response:\n",
        "            response_data = await response.json()  # Get the response data in JSON format\n",
        "            try:\n",
        "                # Extract the response content\n",
        "                resp = response_data['choices'][0]['message']['content']\n",
        "                # Clean the response content\n",
        "                if 'json' in resp or '```' in resp:\n",
        "                    resp = resp.replace('json\\n', '').replace('```\\n', \"\").replace(\n",
        "                        '\\n```', \"\").replace('```', '').replace('json', '').replace('<<emoji>>', '')\n",
        "                    logger.info(\"Lease doc response content cleaned\")\n",
        "            except Exception as error:\n",
        "                # Log any errors that occur during processing\n",
        "                logger.error(f\"Error while cleaning: {error}\")\n",
        "                resp = \"Failed to Transcribe\"\n",
        "\n",
        "    return resp\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prompt to extract textual data from the images and also generate the description of the image of each page\n",
        "extract_data_from_images_prompt = \"\"\"You are a highly capable OCR model that pays great attention to details. Your task is to extract all the text from the input image while maintaining the original layout and also generate a description of the input image if it contains any graph or chart. Output should be in JSON format. Follow these instructions carefully:\n",
        "\n",
        "1. Preserve the original structure, especially in cases when is text arranged in boxes or blocks.\n",
        "2. Ensure all paragraphs are extracted from each section without missing any.\n",
        "3. Do not miss any blocks if the image contains a block layout.\n",
        "4. If the input contains any graph or chart, please explain that under the description key.\n",
        "5. The description must cover all important aspects of the graph or charts or tables.\n",
        "6. Description should not be longer than 250 tokens\n",
        "7. Take your time to analyze your output step by step.\n",
        "8. Output the complete extracted text and graph description.\n",
        "9. Format the output as follows:\n",
        "   json\n",
        "   {\n",
        "     \"page1\": {\n",
        "       \"text\": \"all extracted text\",\n",
        "       \"image_description\": \"description\"\n",
        "     },\n",
        "     \"page2\": {\n",
        "       \"text\": \"all extracted text\",\n",
        "       \"image_description\": \"description\"\n",
        "     }\n",
        "     ...\n",
        "   }\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "y4EmqiKTA_F2"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "GikDlqp-5RVb"
      },
      "outputs": [],
      "source": [
        "#Process a document asynchronously by converting its pages to images, transcribing the content using LLM, and storing the content into the json file\n",
        "async def process_input_document(input_doc_path, extract_data_from_images_prompt,images_folder_path):\n",
        "    \"\"\"\n",
        "    Process a document by converting its pages to images, transcribing the content using OpenAI's GPT model,\n",
        "    and aggregating the transcriptions.\n",
        "\n",
        "    Args:\n",
        "        doc_path (str): Path to the PDF document.\n",
        "        full_prompt (str): Prompt text to be included in the request to the OpenAI API.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the full transcription as a string and a JSON object with batch-wise transcriptions.\n",
        "    \"\"\"\n",
        "    start = time.time()\n",
        "    transcription_for_pages = \"\"\n",
        "    transcription_for_pages_json = {}\n",
        "\n",
        "    doc_name = input_doc_path.split('/')[-1].split(\".\")[0].replace(\" \",\"_\").lower()\n",
        "\n",
        "    try:\n",
        "\n",
        "        # Check if the upload folder exists, if not, create it\n",
        "        images_folder_path = f\"{images_folder_path}/{doc_name}\"\n",
        "        os.makedirs(images_folder_path, exist_ok=True)\n",
        "        # os.makedirs(output_data_file_name,exist_ok=True)\n",
        "\n",
        "\n",
        "        print(\"Procesing the input document asynchronously in the batchs...\")\n",
        "        images_payload = [{\"type\": \"text\", \"text\": extract_data_from_images_prompt}]\n",
        "        batch = 1\n",
        "        tasks = []\n",
        "        with pdfplumber.open(input_doc_path) as pdf:\n",
        "            for i, page in enumerate(pdf.pages):\n",
        "                # if i<4:\n",
        "                k = i + 1\n",
        "                # Convert PDF page to image\n",
        "                image = page.to_image(100)\n",
        "                # Save image to output folder\n",
        "                image_path = f\"{images_folder_path}/page_{k}.png\"\n",
        "                image.save(image_path)\n",
        "\n",
        "                # Getting the base64 string\n",
        "                base64_image = encode_image(image_path)\n",
        "                req_content = {\n",
        "                    \"type\": \"image_url\",\n",
        "                    \"image_url\": {\n",
        "                        \"url\": f\"data:image/png;base64,{base64_image}\",\n",
        "                        \"detail\": \"high\"\n",
        "                    }\n",
        "                }\n",
        "\n",
        "                images_payload.append(req_content)\n",
        "\n",
        "                # Process in batches of 2 images\n",
        "                if k % 2 == 0:\n",
        "\n",
        "                    print(f\"Processing batch#: {batch}\")\n",
        "                    # print(\"Successfully converted PDF to images\", flush=True)\n",
        "\n",
        "                    tasks.append(asyncio.create_task(LLM_call(images_payload)))\n",
        "                    images_payload = [{\"type\": \"text\", \"text\": extract_data_from_images_prompt}]\n",
        "\n",
        "                    batch += 1\n",
        "\n",
        "        final_resp = await asyncio.gather(*tasks)\n",
        "        for batch,resp in enumerate(final_resp):\n",
        "            transcription_for_pages += \"\\n\\n\" + resp\n",
        "\n",
        "            try:\n",
        "                transcription_for_pages_json[f\"batch{batch}\"] = json.loads(resp)\n",
        "            except Exception as error:\n",
        "                logger.error(f\"Error while parsing JSON: {error}\")\n",
        "\n",
        "\n",
        "    except Exception as error:\n",
        "        logger.error(f\"Error: {error}\")\n",
        "        resp = \"Failed to Transcribe\"\n",
        "\n",
        "    transcription_for_pages_json_sorted = {}\n",
        "    page = 1\n",
        "\n",
        "    for pages in transcription_for_pages_json.values():\n",
        "        for i in range(1, 3):\n",
        "            transcription_for_pages_json_sorted[f\"page_{page}\"] = pages[f'page{i}']\n",
        "            page += 1\n",
        "\n",
        "\n",
        "    try:\n",
        "      output_data_file_name = f\"{doc_name}.json\"\n",
        "      # Open the file in write mode and store the JSON data\n",
        "      with open(output_data_file_name, 'w') as json_file:\n",
        "          json.dump(transcription_for_pages_json_sorted, json_file, indent=4)\n",
        "          print(f\"Extracted Data is stored in {output_data_file_name} file\")\n",
        "    except Exception as error:\n",
        "      logger.error(f\"Error while saving the document. Error: {error}\")\n",
        "\n",
        "    logger.info(f\"Lease agreement processing time: {round(float(time.time() - start), 2)} secs\")\n",
        "    return transcription_for_pages, transcription_for_pages_json_sorted\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the process process_input_document with the specified attributes\n",
        "input_doc_path = \"/content/Investment Case For Disruptive Innovation.pdf\"\n",
        "images_folder_path = \"images\"\n",
        "doc_name = input_doc_path.split('/')[-1].split(\".\")[0].replace(\" \",\"_\").lower()\n",
        "final_transcription,final_transcription_json = await process_input_document(input_doc_path, extract_data_from_images_prompt,images_folder_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41CTl7vYAZUT",
        "outputId": "870e57db-bc0c-4850-85ff-29ec7f4c4f43"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Procesing the input document asynchronously in the batchs...\n",
            "Processing batch#: 1\n",
            "Processing batch#: 2\n",
            "Processing batch#: 3\n",
            "Processing batch#: 4\n",
            "Processing batch#: 5\n",
            "Processing batch#: 6\n",
            "Processing batch#: 7\n",
            "Processing batch#: 8\n",
            "Processing batch#: 9\n",
            "Processing batch#: 10\n",
            "Processing batch#: 11\n",
            "Extracted Data is stored in investment_case_for_disruptive_innovation.json file\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmDZ7r-35RVc"
      },
      "source": [
        "### Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "M8Dsyuvw5RVc"
      },
      "outputs": [],
      "source": [
        "#json reading\n",
        "output_data_file_name = f\"{doc_name}.json\"\n",
        "with open(output_data_file_name, 'r') as file:\n",
        "    data = json.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "d-JioOY05RVc"
      },
      "outputs": [],
      "source": [
        "#Intializing the recursive character Text splitter\n",
        "# Create the text splitter\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    model_name=\"gpt-4\",\n",
        "    chunk_size=100,\n",
        "    chunk_overlap=0,\n",
        "    is_separator_regex=[\"\\n\\n\", \".\", \"\\n\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IihpmGn15RVc",
        "outputId": "87fd1e0e-67e4-4c33-8e5d-292b57f60b81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Page#2: total no of chunks#:4\n",
            "  Total tokens in chunk#1: 11\n",
            "  Total tokens in chunk#2: 94\n",
            "  Total tokens in chunk#3: 91\n",
            "  Total tokens in chunk#4: 39\n",
            "\n",
            "Page#3: total no of chunks#:16\n",
            "  Total tokens in chunk#1: 14\n",
            "  Total tokens in chunk#2: 3\n",
            "  Total tokens in chunk#3: 99\n",
            "  Total tokens in chunk#4: 9\n",
            "  Total tokens in chunk#5: 3\n",
            "  Total tokens in chunk#6: 98\n",
            "  Total tokens in chunk#7: 37\n",
            "  Total tokens in chunk#8: 2\n",
            "  Total tokens in chunk#9: 99\n",
            "  Total tokens in chunk#10: 36\n",
            "  Total tokens in chunk#11: 2\n",
            "  Total tokens in chunk#12: 99\n",
            "  Total tokens in chunk#13: 8\n",
            "  Total tokens in chunk#14: 4\n",
            "  Total tokens in chunk#15: 99\n",
            "  Total tokens in chunk#16: 15\n",
            "\n",
            "Page#4: total no of chunks#:5\n",
            "  Total tokens in chunk#1: 49\n",
            "  Total tokens in chunk#2: 98\n",
            "  Total tokens in chunk#3: 3\n",
            "  Total tokens in chunk#4: 99\n",
            "  Total tokens in chunk#5: 20\n",
            "\n",
            "Page#5: total no of chunks#:3\n",
            "  Total tokens in chunk#1: 82\n",
            "  Total tokens in chunk#2: 99\n",
            "  Total tokens in chunk#3: 50\n",
            "\n",
            "Page#6: total no of chunks#:3\n",
            "  Total tokens in chunk#1: 88\n",
            "  Total tokens in chunk#2: 25\n",
            "  Total tokens in chunk#3: 76\n",
            "\n",
            "Page#7: total no of chunks#:2\n",
            "  Total tokens in chunk#1: 95\n",
            "  Total tokens in chunk#2: 99\n",
            "\n",
            "Page#8: total no of chunks#:3\n",
            "  Total tokens in chunk#1: 61\n",
            "  Total tokens in chunk#2: 99\n",
            "  Total tokens in chunk#3: 71\n",
            "\n",
            "Page#9: total no of chunks#:5\n",
            "  Total tokens in chunk#1: 98\n",
            "  Total tokens in chunk#2: 92\n",
            "  Total tokens in chunk#3: 66\n",
            "  Total tokens in chunk#4: 98\n",
            "  Total tokens in chunk#5: 47\n",
            "\n",
            "Page#10: total no of chunks#:5\n",
            "  Total tokens in chunk#1: 12\n",
            "  Total tokens in chunk#2: 98\n",
            "  Total tokens in chunk#3: 9\n",
            "  Total tokens in chunk#4: 87\n",
            "  Total tokens in chunk#5: 51\n",
            "\n",
            "Page#11: total no of chunks#:6\n",
            "  Total tokens in chunk#1: 15\n",
            "  Total tokens in chunk#2: 99\n",
            "  Total tokens in chunk#3: 17\n",
            "  Total tokens in chunk#4: 93\n",
            "  Total tokens in chunk#5: 92\n",
            "  Total tokens in chunk#6: 87\n",
            "\n",
            "Page#12: total no of chunks#:4\n",
            "  Total tokens in chunk#1: 9\n",
            "  Total tokens in chunk#2: 99\n",
            "  Total tokens in chunk#3: 3\n",
            "  Total tokens in chunk#4: 30\n",
            "\n",
            "Page#13: total no of chunks#:3\n",
            "  Total tokens in chunk#1: 79\n",
            "  Total tokens in chunk#2: 95\n",
            "  Total tokens in chunk#3: 65\n",
            "\n",
            "Page#14: total no of chunks#:3\n",
            "  Total tokens in chunk#1: 55\n",
            "  Total tokens in chunk#2: 85\n",
            "  Total tokens in chunk#3: 46\n",
            "\n",
            "Page#15: total no of chunks#:6\n",
            "  Total tokens in chunk#1: 68\n",
            "  Total tokens in chunk#2: 76\n",
            "  Total tokens in chunk#3: 30\n",
            "  Total tokens in chunk#4: 98\n",
            "  Total tokens in chunk#5: 22\n",
            "  Total tokens in chunk#6: 43\n",
            "\n",
            "Page#16: total no of chunks#:3\n",
            "  Total tokens in chunk#1: 88\n",
            "  Total tokens in chunk#2: 61\n",
            "  Total tokens in chunk#3: 41\n",
            "\n",
            "Page#17: total no of chunks#:2\n",
            "  Total tokens in chunk#1: 94\n",
            "  Total tokens in chunk#2: 91\n",
            "\n",
            "Page#18: total no of chunks#:3\n",
            "  Total tokens in chunk#1: 79\n",
            "  Total tokens in chunk#2: 62\n",
            "  Total tokens in chunk#3: 74\n",
            "\n",
            "Page#19: total no of chunks#:5\n",
            "  Total tokens in chunk#1: 98\n",
            "  Total tokens in chunk#2: 87\n",
            "  Total tokens in chunk#3: 33\n",
            "  Total tokens in chunk#4: 76\n",
            "  Total tokens in chunk#5: 42\n",
            "\n",
            "Page#20: total no of chunks#:1\n",
            "  Total tokens in chunk#1: 90\n",
            "\n",
            "Page#21: total no of chunks#:7\n",
            "  Total tokens in chunk#1: 57\n",
            "  Total tokens in chunk#2: 85\n",
            "  Total tokens in chunk#3: 95\n",
            "  Total tokens in chunk#4: 99\n",
            "  Total tokens in chunk#5: 8\n",
            "  Total tokens in chunk#6: 99\n",
            "  Total tokens in chunk#7: 65\n",
            "\n",
            "Page#22: total no of chunks#:11\n",
            "  Total tokens in chunk#1: 84\n",
            "  Total tokens in chunk#2: 97\n",
            "  Total tokens in chunk#3: 100\n",
            "  Total tokens in chunk#4: 100\n",
            "  Total tokens in chunk#5: 99\n",
            "  Total tokens in chunk#6: 34\n",
            "  Total tokens in chunk#7: 99\n",
            "  Total tokens in chunk#8: 53\n",
            "  Total tokens in chunk#9: 46\n",
            "  Total tokens in chunk#10: 88\n",
            "  Total tokens in chunk#11: 10\n"
          ]
        }
      ],
      "source": [
        "total_no_chunks = 0\n",
        "documents = []\n",
        "metadata = []\n",
        "ids = []\n",
        "for page_no, content in data.items():\n",
        "    page_no = page_no.split(\"_\")[-1]\n",
        "    if page_no == '1':\n",
        "        continue\n",
        "    # Split the text\n",
        "    text = content['text']\n",
        "    description = content[\"image_description\"]\n",
        "\n",
        "    text_to_embed = text\n",
        "    chunks = text_splitter.split_text(text_to_embed)\n",
        "    print(f\"\\nPage#{page_no}: total no of chunks#:{len(chunks)}\")\n",
        "    total_no_chunks = total_no_chunks + len(chunks)\n",
        "    for c_id,chunk in enumerate(chunks):\n",
        "        print(f\"  Total tokens in chunk#{c_id+1}: {len(enc.encode(chunk))}\")\n",
        "        documents.append(chunk)\n",
        "        chunk_metadata = {\"chunk_id\":c_id+1,\n",
        "                          \"chunk_text\":chunk,\n",
        "                          \"text\":text,\n",
        "                          \"description\":description,\n",
        "                          \"page_no\":page_no}\n",
        "        metadata.append(chunk_metadata)\n",
        "        ids.append(str(uuid.uuid4()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upsert chunk vectors into pinecone"
      ],
      "metadata": {
        "id": "_pV_SVWClueb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_embeddings(documents,input_type):\n",
        "\n",
        "    # Initialize Voyageai\n",
        "    voyageai.api_key = VOYAGEAI_API_KEY\n",
        "    vo = voyageai.Client()\n",
        "\n",
        "    # Generate embeddings\n",
        "    batch_size = 128\n",
        "    embeddings = []\n",
        "\n",
        "        # logger.info(f\"Generating embeddings of {len(documents)} documents...\")\n",
        "    for i in range(0, len(documents), batch_size):\n",
        "        embeddings += vo.embed(\n",
        "            documents[i:i + batch_size], model=\"voyage-large-2-instruct\", input_type=input_type\n",
        "        ).embeddings\n",
        "\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "l2GFegkVCQ90"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjFjSrdVmyyh",
        "outputId": "af06aa18-2396-4ad7-9fae-ceb0056e7d7f"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['investment_case_for_disruptive_innovation',\n",
              " 'tifin-voyageai',\n",
              " 'Investment_Case_For_Disruptive_Innovation']"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "hUdpbidL5RVc"
      },
      "outputs": [],
      "source": [
        "#VoyageAI embedding push\n",
        "def data_insertion_to_VectorDB(documents,metadata,ids,doc_name):\n",
        "    #generating the data embeddings\n",
        "    voyage_embs = generate_embeddings(documents,\"document\")\n",
        "\n",
        "    #deleting if namespace already created\n",
        "    namespaces = index.describe_index_stats()['namespaces']\n",
        "    namespaces = list(namespaces.keys())\n",
        "    if doc_name in namespaces:\n",
        "      print(\"First deleting all vectors in the namespace\")\n",
        "      index.delete(delete_all=True, namespace=doc_name)\n",
        "\n",
        "    print(f\"Now inserting vectors into the {doc_name} namespace\")\n",
        "    batch_size = 256\n",
        "    for i in range(0, len(voyage_embs), batch_size):\n",
        "        i_end = min(i+batch_size, len(voyage_embs))\n",
        "        batch_ids,batch_embeddings,batch_metadata = ids[i:i_end],voyage_embs[i:i_end],metadata[i:i_end]\n",
        "        to_upsert = list(zip(batch_ids, batch_embeddings, batch_metadata))\n",
        "        index.upsert(vectors=to_upsert, namespace=doc_name)\n",
        "        print(index.describe_index_stats())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(index.describe_index_stats())"
      ],
      "metadata": {
        "id": "aADkpYpinvT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To delete vectors from the index from specific namespace\n",
        "# index.delete(delete_all=True, namespace=doc_name)"
      ],
      "metadata": {
        "id": "5NEZyMtuF0Xs"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H66qjX-f5RVc",
        "outputId": "ab8f5b87-3764-489a-bbef-e39afe7aef86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deleting all vectors in the namespace\n",
            "{'dimension': 1024,\n",
            " 'index_fullness': 0.0,\n",
            " 'namespaces': {'Investment_Case_For_Disruptive_Innovation': {'vector_count': 91},\n",
            "                'tifin-voyageai': {'vector_count': 100}},\n",
            " 'total_vector_count': 191}\n"
          ]
        }
      ],
      "source": [
        "#Generating embs of the chunks and storing them into Vector DB\n",
        "data_insertion_to_VectorDB(documents,metadata,ids,doc_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R65VafSh5RVc",
        "outputId": "701d2d52-07a0-41f4-d2ae-7b17db875919"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1024,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'Investment_Case_For_Disruptive_Innovation': {'vector_count': 91},\n",
              "                'investment_case_for_disruptive_innovation': {'vector_count': 100},\n",
              "                'tifin-voyageai': {'vector_count': 100}},\n",
              " 'total_vector_count': 291}"
            ]
          },
          "metadata": {},
          "execution_count": 99
        }
      ],
      "source": [
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuVBRRMb5RVc"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "F2iCBojM5RVc"
      },
      "outputs": [],
      "source": [
        "def inference(query, QA_prompt, images_folder_path, doc_name, pages, descriptions, chunk_limits):\n",
        "    \"\"\"\n",
        "    Performs inference by sending a query and associated image data to the OpenAI API and returns the response.\n",
        "\n",
        "    Args:\n",
        "        query (str): The user's specific question.\n",
        "        QA_prompt (str): The prompt template for generating the query.\n",
        "        images_folder_path (str): The folder path where images are stored.\n",
        "        doc_name (str): The name of the document containing the images.\n",
        "        pages (list): A list of page numbers to be processed.\n",
        "        descriptions (list): A list of descriptions for each image.\n",
        "        chunk_limits (list): A list of chunk limits to be included in the prompt.\n",
        "\n",
        "    Returns:\n",
        "        str: The processed response content from the OpenAI API.\n",
        "    \"\"\"\n",
        "\n",
        "    # Merge all image descriptions into a single string\n",
        "    merged_description = \"\"\n",
        "    for i, desc in enumerate(descriptions):\n",
        "        merged_description = merged_description + f\"\\nImage_{i+1} description:\\n{desc}\\n\\n\"\n",
        "\n",
        "    # Replace placeholders in the QA prompt with actual query and descriptions\n",
        "    QA_prompt = QA_prompt.replace(\"__query__\", query).replace(\"__description__\", merged_description).replace(\"__chunks__\", \"\\n\\n\".join(chunk_limits))\n",
        "\n",
        "    # Get the API key from user data\n",
        "    api_key = OPENAI_API_KEY\n",
        "    model = \"gpt-4o\"\n",
        "    # Set the headers for the API request\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {api_key}\"\n",
        "    }\n",
        "\n",
        "    # Set the path to the folder containing the images\n",
        "    images_folder_path = f\"{images_folder_path}/{doc_name}\"\n",
        "\n",
        "    # Initialize the payload with the QA prompt\n",
        "    images_payload = [\n",
        "        {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": f\"{QA_prompt}\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Process each page, convert it to base64, and add it to the payload\n",
        "    for page_no in pages:\n",
        "        image_path = f\"{images_folder_path}/page_{page_no}.png\"\n",
        "        base64_image = encode_image(image_path)\n",
        "        req_content = {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "                \"url\": f\"data:image/png;base64,{base64_image}\",\n",
        "                \"detail\": \"high\"\n",
        "            }\n",
        "        }\n",
        "        # print(\"Successfully converted PDF to images\", flush=True)\n",
        "        images_payload.append(req_content)\n",
        "\n",
        "    # Prepare the final payload for the API request\n",
        "    payload = {\n",
        "        \"model\": model,\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": images_payload\n",
        "            }\n",
        "        ],\n",
        "        \"max_tokens\": 1000\n",
        "    }\n",
        "\n",
        "    # Make the API request\n",
        "    response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "    # print(f\"Uncleaned Response:\\n\\n{response}\", flush=True)\n",
        "\n",
        "    # Extract and return the content of the response\n",
        "    return response.json()['choices'][0]['message']['content']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ZmlUHP_N5RVc"
      },
      "outputs": [],
      "source": [
        "def retriever(query,doc_name,verbose):\n",
        "    query_emb = generate_embeddings([query],\"query\")\n",
        "    resp = index.query(namespace=doc_name,\n",
        "                            vector=query_emb[0],\n",
        "                            top_k=5,\n",
        "                            include_metadata=True,\n",
        "                            include_values=False\n",
        "                        )\n",
        "    len(resp['matches'])\n",
        "    # print(resp[\"matches\"])\n",
        "\n",
        "    #Vector Database response cleaning\n",
        "    pages = []\n",
        "    page_text = []\n",
        "    chunk_texts = []\n",
        "    descriptions = []\n",
        "    chunk_ids = []\n",
        "\n",
        "    for match  in resp['matches']:\n",
        "        if match['score']>0.680:\n",
        "            metadata = match[\"metadata\"]\n",
        "            pages.append(metadata['page_no'])\n",
        "            page_text.append(metadata['text'])\n",
        "            descriptions.append(metadata['description'])\n",
        "            chunk_texts.append(metadata['chunk_text'])\n",
        "            chunk_ids.append(int(metadata['chunk_id']))\n",
        "\n",
        "    limit = 3\n",
        "    print(f\"Relevant all pages: {pages}\")\n",
        "    pages = remove_duplicates_preserve_order(pages)[:limit]\n",
        "    descriptions = remove_duplicates_preserve_order(descriptions)[:limit]\n",
        "    total_inputs_vectors = len(pages)\n",
        "    print(f\"Length of input vectors: {total_inputs_vectors}\")\n",
        "    print(f\"Unique Relevant pages: {pages}\")\n",
        "    merged_chunk_texts = '\\n\\n'.join(chunk_texts)\n",
        "    merged_descriptions =  '\\n\\n'.join(descriptions)\n",
        "    if verbose:\n",
        "        print(f\"\\nRelevant Chunks:\\n {merged_chunk_texts}\\n\\n\")\n",
        "        print(f\"\\nRelevant Descriptions:\\n {merged_descriptions}\\n\\n\")\n",
        "    return pages,page_text,chunk_texts[:limit],descriptions,chunk_ids"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "QA_prompt = \"\"\"You are a helpful assistant, your task is to respond to user queries in a clear, and helpful manner, following a U.S. language style. You will be provided with key details like **context**, **query**, **images_descriptions** and **images** to craft your responses, focusing specifically on what the user has asked. Follow the guidelines while responding to user queries.\n",
        "\n",
        "### Information Provided:\n",
        "- **Context**: Relevant information.\n",
        "- **Query**: The user’s specific question.\n",
        "- **images**: images containing relevant information to query.\n",
        "- **images_descriptions**: Detailed description of the images\n",
        "\n",
        "Instructions to follow:\n",
        "\n",
        "1. Take your time, carefully analyze the input images and the accompanying description and context to extract necessary details.\n",
        "2. If the required information is not present in either the input images or the description, refrain from answering the query just say something like provided content not enough information to give answer to the query.\n",
        "3. Ensure your answer is concise and directly addresses the query.\n",
        "4. Do not include any additional information or context outside of what is provided in the input images and description and context.\n",
        "5. Take your time to analyze carefully all the input information especially images.\n",
        "6. Analyze all your steps before giving the final answer.\n",
        "\n",
        "Input:\n",
        "\n",
        "    Query: __query__\n",
        "\n",
        "    Description: __description__\n",
        "\n",
        "    Context: __chunks__\n",
        "\n",
        "Output:\n",
        "\n",
        "    Answer:\"\"\""
      ],
      "metadata": {
        "id": "0XeGipIFG0CO"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGzYmtAu5RVc",
        "outputId": "41303326-3584-4c98-d82c-f5c7c5760eea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: What unique view does ARK have towards Autonomous Mobility and its market potential?\n"
          ]
        }
      ],
      "source": [
        "questions = [\n",
        "    \"What is the core objective of investing in disruptive innovation according to ARK?\",\n",
        "    \"What are the significant risks associated with investing in innovation as highlighted by ARK?\",\n",
        "    \"Can you list the converging innovation platforms identified by ARK?\",\n",
        "    \"How does ARK describe the impact of Artificial Intelligence on technology's integration into economic sectors?\",\n",
        "    \"What transformative potential does Multiomic Sequencing hold according to ARK?\",\n",
        "    \"What are the implications of declining battery technology costs as outlined by ARK?\",\n",
        "    \"How is the field of Robotics anticipated to evolve with the advancements in AI?\",\n",
        "    \"What does the ARK's Convergence Scoring Framework illustrate about innovation platforms?\",\n",
        "    \"How do neural networks serve as a catalyst for other technologies?\",\n",
        "    \"What unique view does ARK have towards Autonomous Mobility and its market potential?\",\n",
        "    \"How do AI Chatbots contribute to the development of robotaxis?\",\n",
        "    \"What are breakthroughs in DNA Sequencing, particularly with neural networks?\",\n",
        "    \"How does the application of AI language models in robotics enhance general task completion rates?\",\n",
        "    \"In what ways are battery advances critical to the future of intelligent devices and augmented reality?\",\n",
        "    \"How do reusable rockets contribute to global connectivity?\",\n",
        "    \"What economic implications do disruptive innovations have according to ARK?\",\n",
        "    \"What are the top 10 holdings of ARK Innovation ETF (ARKK)?\",\n",
        "    \"What thematic strategies do ARK ETFs focus on?\",\n",
        "    \"What is ARK's strategy for capturing the benefits of disruptive innovation in its investment approach?\",\n",
        "    \"How does ARK ensure its investment strategies align with reality of disruptive innovation trends?\"\n",
        "]\n",
        "query = questions[9]\n",
        "print(f\"Question: {query}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MgrFFhx65RVc",
        "outputId": "4043a053-3454-4b45-e679-e1ff8cd2ecdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relevant all pages: ['3', '6', '5', '11', '2']\n",
            "Length of input vectors: 3\n",
            "Unique Relevant pages: ['3', '6', '5']\n",
            "\n",
            "\n",
            "Question:\n",
            "What unique view does ARK have towards Autonomous Mobility and its market potential?\n",
            "\n",
            "Answer:\n",
            "ARK's unique view on Autonomous Mobility highlights that declining costs of advanced battery technology will enable a significant expansion in form factors, facilitating systems that drastically reduce the cost of transporting people and goods. This cost decline is expected to unlock micro-mobility and aerial systems, such as flying taxis, transforming city landscapes. ARK anticipates that autonomy will reduce the costs for taxi, delivery, and surveillance services by an order of magnitude, promoting frictionless transport, boosting e-commerce speed, and reducing individual car ownership.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "verbose = False   # if you want he input relevant chunks and input image descriptions then set verbose=True\n",
        "pages,page_text,chunk_texts,descriptions,chunk_ids = retriever(query,doc_name,verbose)\n",
        "answer = inference(query,QA_prompt,images_folder_path,doc_name,pages,descriptions,chunk_texts)\n",
        "print(f\"\\n\\nQuestion:\\n{query}\\n\\nAnswer:\\n{answer}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hylychat",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}